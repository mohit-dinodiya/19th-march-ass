{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465d6792-6c29-4f08-9c05-247fcdf83cb6",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9182e5-d8db-49b5-a045-76606ac51ebd",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data normalization technique used in data preprocessing to transform the feature values within a fixed range. The method scales the data to a specific range, usually between 0 and 1 or -1 and 1, depending on the requirements of the problem.\n",
    "\n",
    "The formula for Min-Max scaling is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6deac-4ff5-424c-9a23-a2489f937f9b",
   "metadata": {},
   "source": [
    "#### X_scaled = (X - X_min) / (X_max - X_min)\n",
    "#### Age_scaled = (Age - 20) / (60 - 20)\n",
    "#### Age_scaled = (30 - 20) / (60 - 20) = 0.25\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82194c54-8279-49ac-bcc6-15865798340d",
   "metadata": {},
   "source": [
    "where X is the original feature value, X_min and X_max are the minimum and maximum values of the feature, respectively, and X_scaled is the transformed value within the fixed range.\n",
    "\n",
    "For example, suppose we have a dataset containing the age and income of a group of individuals, and we want to normalize the age feature between 0 and 1. We first find the minimum and maximum values of the age feature in the dataset. Suppose the minimum age is 20, and the maximum age is 60. Using the Min-Max scaling formula, we can transform the age values as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b1eeb5-271d-47f0-86b0-571deccce232",
   "metadata": {},
   "source": [
    "Suppose one of the individuals in the dataset has an age of 30. The Min-Max scaling transforms their age value as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec5726-e286-4a4d-9b66-c2b10f9f90a8",
   "metadata": {},
   "source": [
    "So, the transformed value for their age feature is 0.25, which is within the desired range of 0 to 1. The Min-Max scaling can be similarly applied to the other features in the dataset as required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85e352-59c7-4925-9afa-16b6b67ea992",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333b885-9e87-474c-af46-d468797d83da",
   "metadata": {},
   "source": [
    "The Unit Vector technique is a data normalization method used in feature scaling, which scales the feature values to have a magnitude of 1 while maintaining the direction of the original feature vector. The method is also known as vector normalization or L2 normalization.\n",
    "\n",
    "The formula for Unit Vector technique is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b562d-1295-45af-8058-df55f8bb0067",
   "metadata": {},
   "source": [
    "x_normalised = x/|x|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0340b0f-e641-46d6-a7d3-fb127dd093ae",
   "metadata": {},
   "source": [
    "where X is the original feature vector, ||X|| is the magnitude or the length of the vector, and X_normalized is the transformed feature vector with unit magnitude.\n",
    "\n",
    "Compared to Min-Max scaling, which scales the feature values within a fixed range, Unit Vector technique normalizes the feature vector and makes it comparable with other feature vectors that may have different magnitudes.\n",
    "\n",
    "For example, suppose we have a dataset containing the height and weight of a group of individuals, and we want to normalize the height feature using the Unit Vector technique. Suppose the height and weight of one individual are 160 cm and 50 kg, respectively. The original feature vector for this individual is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7d74e-e767-44f1-a90c-b8f9aea245c1",
   "metadata": {},
   "source": [
    "#### x = [160 , 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618420e8-28e4-4ac4-a992-45a0b6911edb",
   "metadata": {},
   "source": [
    "#### x1 = (sqrt(160^2 + 50^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa0f0f-232d-4d44-9186-dda89b2bd1fa",
   "metadata": {},
   "source": [
    "height_normalised = 160/x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d2829-326c-4974-b903-2002b8577d22",
   "metadata": {},
   "source": [
    "So, the transformed value for the height feature is 0.956, which has a unit magnitude. The same method can be applied to normalize the weight feature or other features in the dataset.\n",
    "\n",
    "In summary, the Unit Vector technique is used to normalize the feature vector's magnitude to 1 while preserving the original direction. In contrast, Min-Max scaling scales the feature values within a fixed range, making the features comparable with other features that may have different ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75791d15-153d-41b8-b399-9b27997bb5ba",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eaafb6-a0b7-42f2-bc21-2dd3f9971139",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction by identifying a smaller number of uncorrelated variables or components that explain most of the variance in the data.\n",
    "\n",
    "In PCA, a new set of orthogonal (uncorrelated) variables, called principal components, is created by transforming the original data to a new coordinate system that aligns the axes with the directions of maximum variance in the data. The first principal component captures the direction of maximum variance in the data, and each subsequent component captures the remaining variance in descending order.\n",
    "\n",
    "PCA is used in dimensionality reduction to reduce the number of features in the dataset while retaining as much of the original information as possible. It is especially useful when dealing with high-dimensional data sets, where the number of features is much larger than the number of observations, making it difficult to visualize or analyze the data.\n",
    "\n",
    "An example of PCA's application in dimensionality reduction is reducing the dimensions of a dataset containing information about different car models' features. Suppose the original dataset has ten features, including engine displacement, horsepower, torque, fuel efficiency, and so on, for each car model. PCA can be applied to reduce the number of features to, say, three principal components while retaining most of the original information.\n",
    "\n",
    "The PCA algorithm works as follows:\n",
    "\n",
    "Standardize the data to have a mean of 0 and a standard deviation of 1 to ensure that all variables have equal importance.\n",
    "\n",
    "Compute the covariance matrix or the correlation matrix of the standardized data.\n",
    "\n",
    "Compute the eigenvalues and eigenvectors of the covariance matrix or the correlation matrix.\n",
    "\n",
    "Sort the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "\n",
    "Select the first k eigenvectors with the highest eigenvalues as the principal components.\n",
    "\n",
    "Transform the original data into the new coordinate system defined by the selected principal components.\n",
    "\n",
    "For example, after applying PCA to the car model dataset, we may get three principal components: PC1, PC2, and PC3. These principal components represent new variables that combine some or all of the original ten features, with each principal component capturing the maximum variance in the data.\n",
    "\n",
    "We can use these three principal components to represent each car model in the reduced feature space, which reduces the dimensionality of the dataset from ten to three, making it easier to visualize and analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577cf97-15c2-4da8-9791-686580efb543",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986585b6-8aa4-4459-9aae-76103b69b03e",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a statistical technique used for feature extraction and dimensionality reduction. Feature extraction is the process of selecting or transforming the original features to a smaller set of relevant features that best represent the data's underlying structure. PCA can be used as a feature extraction technique to identify the most important features or variables that contribute the most to the variance in the data.\n",
    "\n",
    "PCA transforms the original features into a new set of uncorrelated variables or principal components that capture most of the variability in the data. The first principal component captures the direction of maximum variance in the data, and each subsequent component captures the remaining variance in descending order. The principal components can be seen as new features that combine or represent some or all of the original features.\n",
    "\n",
    "PCA can be used for feature extraction in various applications such as image processing, text analysis, and signal processing. For example, in image processing, PCA can be used to identify the most relevant features that capture the most significant variability in the images.\n",
    "\n",
    "Suppose we have a dataset of grayscale images of handwritten digits (0-9). Each image is represented as a vector of pixel values. Each pixel represents a feature, and the dataset has a high dimensionality due to the large number of pixels per image.\n",
    "\n",
    "We can use PCA to extract the most significant features or principal components that best represent the images' underlying structure. We can apply PCA to the dataset and obtain a set of principal components that capture most of the variability in the data.\n",
    "\n",
    "The principal components can be used as a reduced set of features to represent the images, and each image can be reconstructed using a weighted combination of the principal components. By using only the principal components that capture most of the variability in the data, we can reduce the dimensionality of the dataset while retaining most of the original information.\n",
    "\n",
    "In summary, PCA can be used as a feature extraction technique to identify the most relevant features or variables that contribute the most to the variance in the data. The extracted features or principal components can be used as a reduced set of features to represent the data, enabling dimensionality reduction while retaining most of the original information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e38e0-f138-4cfd-bfae-b92e9b7ecdfc",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7a8c3-9c0f-47da-9b04-30a9a70e947d",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data by scaling the features to a common range of values. This is important because the features may have different ranges, and some features may dominate the others, leading to biased recommendations.\n",
    "\n",
    "To use Min-Max scaling, we first determine the minimum and maximum values for each feature in the dataset. Then, we transform each feature using the following formula:\n",
    "\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "This formula scales the feature values to a range between 0 and 1, where 0 represents the minimum value, and 1 represents the maximum value.\n",
    "\n",
    "For example, suppose we have a dataset with features such as price, rating, and delivery time. We can use Min-Max scaling to preprocess the data as follows:\n",
    "\n",
    "Determine the minimum and maximum values for each feature. For instance, the minimum and maximum values for the price feature could be $5 and $50, respectively.\n",
    "\n",
    "Transform each feature using the formula above. For instance, if the original price value is $20, the scaled value would be (20-5) / (50-5) = 0.32.\n",
    "\n",
    "Repeat the transformation for all the features in the dataset.\n",
    "\n",
    "After scaling the features, all features will have a common range of values between 0 and 1. This ensures that no feature dominates the others, and the recommendation system can make unbiased recommendations based on all the features.\n",
    "\n",
    "In summary, using Min-Max scaling is a crucial step in preprocessing data for building a recommendation system. It ensures that all features have a common range of values and prevents biased recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d487e-698d-4b0b-b840-20ec86238d1f",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c225d-5f60-4eaa-b018-e3d53793e034",
   "metadata": {},
   "source": [
    "In the context of building a model to predict stock prices, PCA can be used to reduce the dimensionality of the dataset by identifying the most relevant features that contribute the most to the variance in the data. This is important because high-dimensional datasets can be challenging to analyze, and some features may be redundant, leading to overfitting.\n",
    "\n",
    "To use PCA, we first standardize the dataset to have a mean of 0 and a standard deviation of 1. This is important because PCA is sensitive to the scale of the features, and standardizing the data ensures that all features have a similar scale.\n",
    "\n",
    "Then, we apply PCA to the standardized dataset and obtain a set of principal components that capture most of the variability in the data. We can use the following steps to apply PCA:\n",
    "\n",
    "Compute the covariance matrix of the standardized dataset.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "\n",
    "Select the top k eigenvectors that capture most of the variability in the data. The number of principal components to select, k, can be determined based on the explained variance ratio. The explained variance ratio is the proportion of the total variance in the data that is explained by each principal component.\n",
    "\n",
    "Transform the original dataset into a new dataset using the selected principal components.\n",
    "\n",
    "The new dataset has a reduced dimensionality, where each instance is represented by a set of k principal components instead of the original features. The principal components can be seen as a new set of features that combine or represent some or all of the original features.\n",
    "\n",
    "For example, suppose we have a dataset with many features, such as company financial data and market trends. We can use PCA to reduce the dimensionality of the dataset as follows:\n",
    "\n",
    "Standardize the dataset to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Compute the covariance matrix of the standardized dataset.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "\n",
    "Select the top k eigenvectors that capture most of the variability in the data.\n",
    "\n",
    "Transform the original dataset into a new dataset using the selected principal components.\n",
    "\n",
    "After applying PCA, the new dataset will have a reduced dimensionality, where each instance is represented by a set of k principal components that capture most of the variability in the data. This can improve the model's performance by reducing overfitting and improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be981ece-c896-43e9-8d0a-96fa3ec4c07d",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "280a83f9-6fad-4c58-b879-c9ad738a9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,5,10,15,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25ea1c52-281f-45a8-92d4-1d07a3d86bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80e98d72-980d-4711-a457-9629fe314cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cd50345-cfae-4e70-b08f-f2d0e6b06300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit_transform([[1,5,10,15,20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2eaed82-9480-4794-bc0c-f84522a03473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.transform([[1,5,10,15,20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac48e94-c956-4527-b7b6-ac9019d88ffb",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcfc311-1d59-448d-a233-22fc5d5e713b",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on a dataset containing the features height, weight, age, gender, and blood pressure, we would first standardize the data by subtracting the mean and dividing by the standard deviation of each feature. Then, we would compute the covariance matrix of the standardized data and perform eigendecomposition on this matrix to obtain the principal components.\n",
    "\n",
    "The number of principal components to retain depends on the amount of variance explained by each component. We would typically choose to retain enough principal components to capture a significant amount of the variance in the data while reducing the dimensionality of the dataset.\n",
    "\n",
    "To determine how many principal components to retain, we can examine the explained variance ratio for each component. The explained variance ratio for a principal component is the proportion of the total variance in the data that is explained by that component. We can choose to retain enough components such that the cumulative explained variance ratio is above a certain threshold, such as 80% or 90%.\n",
    "\n",
    "For example, if we find that the first two principal components explain 70% and 20% of the total variance in the data, respectively, we might choose to retain only the first two components, since they capture a significant amount of the variance in the data.\n",
    "\n",
    "The number of principal components to retain can also depend on the specific problem and the desired level of accuracy in the model. In some cases, retaining more principal components can lead to better performance, while in other cases, retaining fewer components may be sufficient.\n",
    "\n",
    "Without knowing the specifics of the dataset and the problem at hand, it is difficult to say how many principal components should be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf56e95-3786-495b-8db2-d94708a7673a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
